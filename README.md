# ğŸ¤ Real-Time MULTILINGUAL Emotion Detection from Speech Signals

## ğŸ“Œ Project Overview
This project is a *Real-Time Emotion Detection System* that analyzes emotions from voice recordings using *Flask (Python) & Node.js* for backend processing and *React.js* for frontend visualization. The system transcribes audio, translates non-English text, and classifies emotions using a pre-trained *RoBERTa-based model*.

## ğŸš€ Features
- ğŸ™ *Speech to Text*: Converts voice recordings to text using Google Speech Recognition.
- ğŸŒ *Multilingual Support*: Detects and translates non-English text to English using Google Translate.
- ğŸ¤– *Emotion Analysis*: Uses the roberta-base-go_emotions model to classify emotions with confidence scores.
- ğŸ–¥ *Frontend Dashboard*: Displays detected emotions in an intuitive UI.
- ğŸ”„ *Real-time Processing*: Uploads audio files, processes them, and returns results instantly.

## ğŸ›  Tech Stack
- *Backend*: Flask (Python), Node.js (Express), Google Speech Recognition, Google Translate API, Hugging Face Transformers
- *Frontend*: React.js, Tailwind CSS, TypeScript (for some components)
- *Deployment*: Flask API (Backend), React (Frontend), GitHub

## ğŸ“‚ Project Structure

ğŸ“¦ Emotion-Detection-Voice
â”œâ”€â”€ backend/ (Node.js & Flask API)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â”‚   â”œâ”€â”€ audiocontroller.js
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ audioroutes.js
â”‚   â”‚   â”‚   â”œâ”€â”€ musicroutes.js
â”‚   â”‚   â”œâ”€â”€ index.js (Backend entry point)
â”‚   â”œâ”€â”€ flask_api/
â”‚   â”‚   â”œâ”€â”€ emotion.py (Emotion classification logic)
â”‚   â”œâ”€â”€ uploads/ (Stores audio files temporarily)
â”‚   â”œâ”€â”€ .env (Environment variables)
â”‚   â”œâ”€â”€ package.json (Node.js dependencies)
â”œâ”€â”€ frontend/ (React.js UI)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ AudioAnalyzer.js
â”‚   â”‚   â”‚   â”œâ”€â”€ AudioVisualizer.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ EmotionResult.js
â”‚   â”‚   â”‚   â”œâ”€â”€ EmotionTrends.js
â”‚   â”‚   â”‚   â”œâ”€â”€ Recorder.js
â”‚   â”‚   â”‚   â”œâ”€â”€ StopRecordingButton.tsx
â”‚   â”‚   â”œâ”€â”€ App.js (Main app logic)
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json (React dependencies)
â”‚   â”œâ”€â”€ index.js (React entry point)
â”œâ”€â”€ README.md (Project Documentation)


## ğŸ”§ Setup & Installation
### 1ï¸âƒ£ Clone the Repository
sh
git clone https://github.com/your-username/your-repo-name.git
cd Emotion-Detection-Voice


### 2ï¸âƒ£ Backend Setup (Node.js & Flask API)
sh
cd backend
# Setup Node.js backend
npm install
npm start  # Run Express server

# Setup Flask API
cd flask_api
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
python emotion.py  # Run Flask server


### 3ï¸âƒ£ Frontend Setup (React.js)
sh
cd frontend
npm install
npm start  # Run React UI


## ğŸ¯ API Endpoints
### ğŸ™ Upload Audio for Emotion Detection
*Endpoint:* POST /predict
*Request:* Upload an audio file as FormData
*Response:*
json
{
  "original_transcription": "I'm feeling very happy today!",
  "translated_text": "I'm feeling very happy today!",
  "emotions": [
    { "label": "joy", "score": 0.98 },
    { "label": "optimism", "score": 0.75 },
    { "label": "love", "score": 0.68 }
  ]
}


## ğŸ›  Deployment (GitHub)
### Push to GitHub
sh
git add .
git commit -m "Initial commit - Flask & Node.js emotion detection with React"
git branch -M main
git remote add origin https://github.com/your-username/your-repo-name.git
git push -u origin main


